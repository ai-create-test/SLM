"""
Tokenizeræ¨¡å—å®Œæ•´å•å…ƒæµ‹è¯•

è¦†ç›–èŒƒå›´ï¼š
1. TokenizerResult æ•°æ®ç»“æ„
2. BaseTokenizer æŠ½è±¡æ¥å£
3. BPETokenizer ç¼–è§£ç åŠŸèƒ½
4. TokenAttentionMixin æ³¨æ„åŠ›æƒé‡è®¡ç®—
5. TokenizerFactory å·¥å‚æ¨¡å¼
6. è¾¹ç•Œæƒ…å†µå’Œé”™è¯¯å¤„ç†
"""

import pytest
import json
import tempfile
import os
from pathlib import Path

# å¯¼å…¥å¾…æµ‹è¯•æ¨¡å—
from app.core import (
    BaseTokenizer,
    TokenizerResult,
    BatchTokenizerResult,
    BPETokenizer,
    TokenizerFactory,
    get_tokenizer,
    TokenAttentionMixin,
    AttentionConfig,
    AttentionStrategy,
    QueryFocusedAttention,
    PaddingStrategy,
    TruncationStrategy,
)


# ==================== TokenizerResult æµ‹è¯• ====================

class TestTokenizerResult:
    """æµ‹è¯•TokenizerResultæ•°æ®ç»“æ„"""
    
    def test_basic_creation(self):
        """æµ‹è¯•åŸºæœ¬åˆ›å»º"""
        result = TokenizerResult(
            token_ids=[1, 2, 3],
            tokens=["hello", "world", "!"]
        )
        assert result.token_ids == [1, 2, 3]
        assert result.tokens == ["hello", "world", "!"]
        assert result.attention_weights is None
        assert result.attention_mask is None
    
    def test_len(self):
        """æµ‹è¯•é•¿åº¦è®¡ç®—"""
        result = TokenizerResult(
            token_ids=[1, 2, 3, 4, 5],
            tokens=["a", "b", "c", "d", "e"]
        )
        assert len(result) == 5
    
    def test_to_dict(self):
        """æµ‹è¯•å­—å…¸è½¬æ¢"""
        result = TokenizerResult(
            token_ids=[1, 2],
            tokens=["a", "b"],
            attention_weights=[0.5, 0.8],
            attention_mask=[1, 1],
            metadata={"test": "value"}
        )
        d = result.to_dict()
        assert d["token_ids"] == [1, 2]
        assert d["tokens"] == ["a", "b"]
        assert d["attention_weights"] == [0.5, 0.8]
        assert d["attention_mask"] == [1, 1]
        assert d["metadata"] == {"test": "value"}
    
    def test_to_dict_optional_fields(self):
        """æµ‹è¯•å¯é€‰å­—æ®µçš„å­—å…¸è½¬æ¢"""
        result = TokenizerResult(token_ids=[1], tokens=["a"])
        d = result.to_dict()
        assert "attention_weights" not in d
        assert "attention_mask" not in d
        assert "metadata" not in d  # ç©ºå­—å…¸ä¸åº”è¯¥å‡ºç°


class TestBatchTokenizerResult:
    """æµ‹è¯•BatchTokenizerResult"""
    
    def test_basic_creation(self):
        """æµ‹è¯•åŸºæœ¬åˆ›å»º"""
        result = BatchTokenizerResult(
            token_ids=[[1, 2], [3, 4, 5]],
            tokens=[["a", "b"], ["c", "d", "e"]]
        )
        assert len(result) == 2
    
    def test_indexing(self):
        """æµ‹è¯•ç´¢å¼•è®¿é—®"""
        result = BatchTokenizerResult(
            token_ids=[[1, 2], [3, 4]],
            tokens=[["a", "b"], ["c", "d"]],
            attention_mask=[[1, 1], [1, 0]]
        )
        single = result[0]
        assert isinstance(single, TokenizerResult)
        assert single.token_ids == [1, 2]
        assert single.tokens == ["a", "b"]
        assert single.attention_mask == [1, 1]


# ==================== BPETokenizer æµ‹è¯• ====================

class TestBPETokenizer:
    """æµ‹è¯•BPETokenizer"""
    
    @pytest.fixture
    def tokenizer(self):
        """åˆ›å»ºé»˜è®¤åˆ†è¯å™¨"""
        return BPETokenizer(encoding_name="cl100k_base")
    
    def test_initialization(self, tokenizer):
        """æµ‹è¯•åˆå§‹åŒ–"""
        assert tokenizer.vocab_size > 0
        assert tokenizer._tiktoken_encoder is not None
    
    def test_basic_encode(self, tokenizer):
        """æµ‹è¯•åŸºæœ¬ç¼–ç """
        result = tokenizer.encode("Hello, world!")
        assert len(result.token_ids) > 0
        assert len(result.tokens) == len(result.token_ids)
        assert result.attention_mask is not None
        assert all(m == 1 for m in result.attention_mask)
    
    def test_encode_without_special_tokens(self, tokenizer):
        """æµ‹è¯•ä¸æ·»åŠ ç‰¹æ®Štokençš„ç¼–ç """
        with_special = tokenizer.encode("test", add_special_tokens=True)
        without_special = tokenizer.encode("test", add_special_tokens=False)
        
        # æœ‰ç‰¹æ®Štokenæ—¶é•¿åº¦åº”è¯¥æ›´é•¿
        assert len(with_special.tokens) > len(without_special.tokens)
        assert "[CLS]" in with_special.tokens
        assert "[CLS]" not in without_special.tokens
    
    def test_decode(self, tokenizer):
        """æµ‹è¯•è§£ç """
        original = "Hello world"
        result = tokenizer.encode(original, add_special_tokens=False)
        decoded = tokenizer.decode(result.token_ids)
        assert decoded == original
    
    def test_encode_decode_roundtrip(self, tokenizer):
        """æµ‹è¯•ç¼–è§£ç å¾€è¿”ä¸€è‡´æ€§"""
        texts = [
            "Simple text",
            "Numbers: 123456",
            "Special chars: @#$%",
            "Unicode: ä½ å¥½ä¸–ç•Œ",
            "Mixed: Hello ä¸–ç•Œ 123!",
        ]
        for text in texts:
            result = tokenizer.encode(text, add_special_tokens=False)
            decoded = tokenizer.decode(result.token_ids)
            assert decoded == text, f"Roundtrip failed for: {text}"
    
    def test_truncation(self, tokenizer):
        """æµ‹è¯•æˆªæ–­åŠŸèƒ½"""
        long_text = "word " * 100
        result = tokenizer.encode(
            long_text, 
            max_length=10, 
            truncation=True,
            add_special_tokens=False
        )
        assert len(result.token_ids) <= 10
    
    def test_padding(self, tokenizer):
        """æµ‹è¯•å¡«å……åŠŸèƒ½"""
        result = tokenizer.encode(
            "short",
            max_length=20,
            padding=True,
            add_special_tokens=False
        )
        assert len(result.token_ids) == 20
        assert result.attention_mask is not None
        # å¡«å……éƒ¨åˆ†çš„maskåº”è¯¥æ˜¯0
        assert 0 in result.attention_mask
    
    def test_encode_with_query(self, tokenizer):
        """æµ‹è¯•å¸¦é—®é¢˜çš„ç¼–ç """
        result = tokenizer.encode(
            "The weather is sunny today.",
            query="What is the weather?"
        )
        assert result.attention_weights is not None
        assert len(result.attention_weights) == len(result.tokens)
        # æƒé‡åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        assert all(0 <= w <= 2.0 for w in result.attention_weights)
    
    def test_encode_batch(self, tokenizer):
        """æµ‹è¯•æ‰¹é‡ç¼–ç """
        texts = ["Hello", "World", "Test"]
        result = tokenizer.encode_batch(texts, add_special_tokens=False)
        
        assert len(result) == 3
        assert len(result.token_ids) == 3
    
    def test_decode_batch(self, tokenizer):
        """æµ‹è¯•æ‰¹é‡è§£ç """
        texts = ["Hello", "World"]
        encoded = tokenizer.encode_batch(texts, add_special_tokens=False)
        decoded = tokenizer.decode_batch(encoded.token_ids)
        
        assert decoded == texts
    
    def test_empty_string(self, tokenizer):
        """æµ‹è¯•ç©ºå­—ç¬¦ä¸²å¤„ç†"""
        result = tokenizer.encode("", add_special_tokens=False)
        assert result.token_ids == []
        assert result.tokens == []
    
    def test_repr(self, tokenizer):
        """æµ‹è¯•å­—ç¬¦ä¸²è¡¨ç¤º"""
        repr_str = repr(tokenizer)
        assert "BPETokenizer" in repr_str
        assert "cl100k_base" in repr_str


class TestBPETokenizerCustom:
    """æµ‹è¯•è‡ªå®šä¹‰BPEåˆ†è¯å™¨"""
    
    def test_custom_tokenizer_initialization(self):
        """æµ‹è¯•è‡ªå®šä¹‰åˆ†è¯å™¨åˆå§‹åŒ–"""
        # ä¸ä½¿ç”¨tiktokenï¼Œä½¿ç”¨é»˜è®¤è¯è¡¨
        tokenizer = BPETokenizer(encoding_name=None)
        assert tokenizer.vocab_size > 0
    
    def test_train_on_corpus(self):
        """æµ‹è¯•åœ¨è¯­æ–™ä¸Šè®­ç»ƒ"""
        tokenizer = BPETokenizer(encoding_name=None)
        corpus = [
            "hello world",
            "hello there",
            "world hello",
        ]
        tokenizer.train(corpus, vocab_size=30, min_frequency=1, show_progress=False)
        
        assert tokenizer.vocab_size >= 10
        assert len(tokenizer._merges) > 0
    
    def test_save_and_load(self):
        """æµ‹è¯•ä¿å­˜å’ŒåŠ è½½"""
        with tempfile.TemporaryDirectory() as tmpdir:
            # åˆ›å»ºå¹¶ä¿å­˜
            tokenizer = BPETokenizer(encoding_name="cl100k_base")
            tokenizer.save(tmpdir)
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            assert os.path.exists(os.path.join(tmpdir, "config.json"))
            
            # åŠ è½½
            loaded = BPETokenizer.load(tmpdir)
            assert loaded.vocab_size == tokenizer.vocab_size


# ==================== TokenAttentionMixin æµ‹è¯• ====================

class TestTokenAttentionMixin:
    """æµ‹è¯•æ³¨æ„åŠ›æƒé‡è®¡ç®—"""
    
    @pytest.fixture
    def tokenizer(self):
        """åˆ›å»ºå¸¦æ³¨æ„åŠ›åŠŸèƒ½çš„åˆ†è¯å™¨"""
        return BPETokenizer(encoding_name="cl100k_base")
    
    def test_keyword_match_weights(self, tokenizer):
        """æµ‹è¯•å…³é”®è¯åŒ¹é…ç­–ç•¥"""
        doc_tokens = ["the", "weather", "is", "sunny", "today"]
        weights = tokenizer.compute_attention_weights(
            doc_tokens, 
            "weather",
            strategy=AttentionStrategy.KEYWORD_MATCH
        )
        
        assert len(weights) == len(doc_tokens)
        # "weather" åº”è¯¥æœ‰æœ€é«˜æƒé‡
        weather_idx = doc_tokens.index("weather")
        assert weights[weather_idx] >= max(weights[i] for i in range(len(doc_tokens)) if i != weather_idx)
    
    def test_bm25_weights(self, tokenizer):
        """æµ‹è¯•BM25ç­–ç•¥"""
        doc_tokens = ["python", "is", "a", "programming", "language"]
        weights = tokenizer.compute_attention_weights(
            doc_tokens,
            "What is Python?",
            strategy=AttentionStrategy.BM25
        )
        
        assert len(weights) == len(doc_tokens)
        # Pythonå’Œisåº”è¯¥æœ‰è¾ƒé«˜æƒé‡
        python_idx = doc_tokens.index("python")
        assert weights[python_idx] > tokenizer.attention_config.min_weight
    
    def test_tfidf_weights(self, tokenizer):
        """æµ‹è¯•TF-IDFç­–ç•¥"""
        doc_tokens = ["machine", "learning", "is", "powerful"]
        weights = tokenizer.compute_attention_weights(
            doc_tokens,
            "learning",
            strategy=AttentionStrategy.TFIDF
        )
        
        assert len(weights) == len(doc_tokens)
    
    def test_attention_config(self):
        """æµ‹è¯•è‡ªå®šä¹‰æ³¨æ„åŠ›é…ç½®"""
        config = AttentionConfig(
            strategy=AttentionStrategy.KEYWORD_MATCH,
            normalize=True,
            min_weight=0.2,
            boost_factor=3.0
        )
        tokenizer = BPETokenizer(
            encoding_name="cl100k_base",
            attention_config=config
        )
        
        doc_tokens = ["hello", "world"]
        weights = tokenizer.compute_attention_weights(doc_tokens, "hello")
        
        # æœ€å°æƒé‡åº”è¯¥æ˜¯0.2
        assert all(w >= 0.2 for w in weights)
    
    def test_update_document_statistics(self, tokenizer):
        """æµ‹è¯•æ–‡æ¡£ç»Ÿè®¡æ›´æ–°"""
        documents = [
            ["hello", "world"],
            ["hello", "python"],
            ["world", "python", "programming"],
        ]
        tokenizer.update_document_statistics(documents)
        
        assert tokenizer._total_documents == 3
        assert tokenizer._avg_doc_length > 0
        assert "hello" in tokenizer._document_frequencies


class TestQueryFocusedAttention:
    """æµ‹è¯•ç‹¬ç«‹çš„æ³¨æ„åŠ›è®¡ç®—å™¨"""
    
    def test_basic_compute(self):
        """æµ‹è¯•åŸºæœ¬è®¡ç®—"""
        calculator = QueryFocusedAttention()
        weights = calculator.compute(
            ["the", "cat", "sat"],
            "cat"
        )
        assert len(weights) == 3
    
    def test_apply_weights(self):
        """æµ‹è¯•æƒé‡åº”ç”¨åˆ°embeddings"""
        calculator = QueryFocusedAttention()
        embeddings = [[1.0, 2.0], [3.0, 4.0]]
        weights = [0.5, 1.0]
        
        weighted = calculator.apply_weights(embeddings, weights)
        
        assert weighted[0] == [0.5, 1.0]
        assert weighted[1] == [3.0, 4.0]
    
    def test_apply_weights_length_mismatch(self):
        """æµ‹è¯•é•¿åº¦ä¸åŒ¹é…æ—¶çš„é”™è¯¯å¤„ç†"""
        calculator = QueryFocusedAttention()
        embeddings = [[1.0, 2.0], [3.0, 4.0]]
        weights = [0.5]  # é•¿åº¦ä¸åŒ¹é…
        
        with pytest.raises(ValueError):
            calculator.apply_weights(embeddings, weights)


# ==================== TokenizerFactory æµ‹è¯• ====================

class TestTokenizerFactory:
    """æµ‹è¯•åˆ†è¯å™¨å·¥å‚"""
    
    def test_create_bpe(self):
        """æµ‹è¯•åˆ›å»ºBPEåˆ†è¯å™¨"""
        tokenizer = TokenizerFactory.create("bpe")
        assert isinstance(tokenizer, BPETokenizer)
    
    def test_create_with_shortcut(self):
        """æµ‹è¯•ä½¿ç”¨å¿«æ·æ–¹å¼åˆ›å»º"""
        tokenizer = TokenizerFactory.create("gpt4")
        assert isinstance(tokenizer, BPETokenizer)
    
    def test_create_unknown_type(self):
        """æµ‹è¯•åˆ›å»ºæœªçŸ¥ç±»å‹æ—¶çš„é”™è¯¯"""
        with pytest.raises(ValueError) as excinfo:
            TokenizerFactory.create("unknown_type")
        assert "æœªçŸ¥çš„åˆ†è¯å™¨ç±»å‹" in str(excinfo.value)
    
    def test_register_and_create(self):
        """æµ‹è¯•æ³¨å†Œå’Œåˆ›å»ºè‡ªå®šä¹‰åˆ†è¯å™¨"""
        # æ³¨å†Œ
        class CustomTokenizer(BPETokenizer):
            pass
        
        TokenizerFactory.register("custom", CustomTokenizer)
        
        # åˆ›å»º
        tokenizer = TokenizerFactory.create("custom")
        assert isinstance(tokenizer, CustomTokenizer)
        
        # æ¸…ç†
        TokenizerFactory.unregister("custom")
    
    def test_register_invalid_class(self):
        """æµ‹è¯•æ³¨å†Œæ— æ•ˆç±»æ—¶çš„é”™è¯¯"""
        class NotATokenizer:
            pass
        
        with pytest.raises(TypeError):
            TokenizerFactory.register("invalid", NotATokenizer)
    
    def test_list_available(self):
        """æµ‹è¯•åˆ—å‡ºå¯ç”¨åˆ†è¯å™¨"""
        available = TokenizerFactory.list_available()
        
        assert "bpe" in available
        assert "gpt4" in available
        assert len(available) >= 4
    
    def test_from_config(self):
        """æµ‹è¯•ä»é…ç½®æ–‡ä»¶åˆ›å»º"""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump({
                "type": "bpe",
                "params": {"encoding_name": "cl100k_base"}
            }, f)
            config_path = f.name
        
        try:
            tokenizer = TokenizerFactory.from_config(config_path)
            assert isinstance(tokenizer, BPETokenizer)
        finally:
            os.unlink(config_path)
    
    def test_get_tokenizer_function(self):
        """æµ‹è¯•ä¾¿æ·å‡½æ•°"""
        tokenizer = get_tokenizer("gpt4")
        assert isinstance(tokenizer, BPETokenizer)


# ==================== è¾¹ç•Œæƒ…å†µå’Œé”™è¯¯å¤„ç†æµ‹è¯• ====================

class TestEdgeCases:
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    
    @pytest.fixture
    def tokenizer(self):
        return BPETokenizer(encoding_name="cl100k_base")
    
    def test_very_long_text(self, tokenizer):
        """æµ‹è¯•éå¸¸é•¿çš„æ–‡æœ¬"""
        long_text = "word " * 10000
        result = tokenizer.encode(long_text, add_special_tokens=False)
        assert len(result.token_ids) > 0
    
    def test_special_characters(self, tokenizer):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦"""
        text = "Hello! @#$%^&*()_+-=[]{}|;':\",./<>?"
        result = tokenizer.encode(text, add_special_tokens=False)
        decoded = tokenizer.decode(result.token_ids)
        assert decoded == text
    
    def test_unicode_text(self, tokenizer):
        """æµ‹è¯•Unicodeæ–‡æœ¬"""
        texts = [
            "ä¸­æ–‡æµ‹è¯•",
            "æ—¥æœ¬èªãƒ†ã‚¹ãƒˆ",
            "í•œêµ­ì–´ í…ŒìŠ¤íŠ¸",
            "Ù…Ø±Ø­Ø¨Ø§",
            "ğŸš€ğŸ’»ğŸ‰",
        ]
        for text in texts:
            result = tokenizer.encode(text, add_special_tokens=False)
            decoded = tokenizer.decode(result.token_ids)
            assert decoded == text, f"Failed for: {text}"
    
    def test_whitespace_handling(self, tokenizer):
        """æµ‹è¯•ç©ºç™½å­—ç¬¦å¤„ç†"""
        text = "  multiple   spaces   "
        result = tokenizer.encode(text, add_special_tokens=False)
        decoded = tokenizer.decode(result.token_ids)
        # è§£ç ååº”è¯¥ä¿æŒä¸€è‡´
        assert text in decoded or decoded.strip() == text.strip()
    
    def test_query_with_no_match(self, tokenizer):
        """æµ‹è¯•é—®é¢˜ä¸æ–‡æ¡£æ— åŒ¹é…æ—¶"""
        result = tokenizer.encode(
            "The cat sat on the mat",
            query="quantum physics"
        )
        # æ²¡æœ‰åŒ¹é…æ—¶ï¼Œæ‰€æœ‰æƒé‡åº”è¯¥ç›¸ç­‰ï¼ˆéƒ½æ˜¯å½’ä¸€åŒ–åçš„å€¼ï¼‰
        assert result.attention_weights is not None
        weights = result.attention_weights
        # æ£€æŸ¥æ‰€æœ‰æƒé‡å€¼ç›¸åŒï¼ˆå› ä¸ºæ²¡æœ‰åŒ¹é…ï¼Œéƒ½åº”è¯¥æ˜¯ç›¸åŒçš„åŸºç¡€æƒé‡ï¼‰
        non_special_weights = [w for w in weights if w is not None]
        assert len(set(non_special_weights)) <= 2  # æœ€å¤š2ç§ä¸åŒå€¼ï¼ˆç‰¹æ®Štokenå¯èƒ½ä¸åŒï¼‰


# ==================== é›†æˆæµ‹è¯• ====================

class TestIntegration:
    """é›†æˆæµ‹è¯•"""
    
    def test_full_workflow(self):
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
        # 1. ä½¿ç”¨å·¥å‚åˆ›å»ºåˆ†è¯å™¨
        tokenizer = get_tokenizer("gpt4")
        
        # 2. ç¼–ç æ–‡æœ¬
        text = "Machine learning is transforming the world."
        query = "What is machine learning?"
        result = tokenizer.encode(text, query=query)
        
        # 3. éªŒè¯ç»“æœ
        assert len(result.token_ids) > 0
        assert result.attention_weights is not None
        assert result.attention_mask is not None
        assert result.metadata.get("encoding") == "cl100k_base"
        
        # 4. è§£ç éªŒè¯
        decoded = tokenizer.decode(result.token_ids)
        assert "machine" in decoded.lower()
    
    def test_batch_workflow(self):
        """æµ‹è¯•æ‰¹é‡å¤„ç†å·¥ä½œæµç¨‹"""
        tokenizer = get_tokenizer("gpt4")
        
        texts = [
            "First document about Python",
            "Second document about Java",
            "Third document about JavaScript",
        ]
        queries = [
            "Python programming",
            "Java development",
            "JavaScript frameworks",
        ]
        
        results = tokenizer.encode_batch(texts, queries=queries)
        
        assert len(results) == 3
        for i in range(3):
            single = results[i]
            assert single.attention_weights is not None


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
